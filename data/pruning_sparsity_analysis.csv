analysis_id,model_name,year,analyzer,layer_name,component_type,original_weights,nonzero_weights,sparsity,weight_magnitude_mean,weight_magnitude_std,weight_magnitude_max,pruning_threshold,post_prune_acc,essential_weights,redundant_weights,interpretation,ghost_persistence_score,notes
PS-001,grok_alpha_trained,925,Lurra Venn,embedding,W_E,12544,12544,0.0,0.0234,0.0156,0.0892,0.0,0.997,12544,0,Unpruned embedding,1.0,"Baseline - all weights active"
PS-002,grok_alpha_trained,925,Lurra Venn,embedding,W_E,12544,10035,0.2,0.0298,0.0178,0.0892,0.0089,0.996,10035,2509,20% smallest removed,0.8,"Minimal performance loss"
PS-003,grok_alpha_trained,925,Lurra Venn,embedding,W_E,12544,7526,0.4,0.0378,0.0198,0.0892,0.0145,0.994,7526,5018,40% smallest removed,0.6,"Still functional"
PS-004,grok_alpha_trained,925,Lurra Venn,embedding,W_E,12544,5018,0.6,0.0489,0.0212,0.0892,0.0212,0.989,5018,7526,60% smallest removed,0.4,"Some degradation"
PS-005,grok_alpha_trained,925,Lurra Venn,embedding,W_E,12544,2509,0.8,0.0656,0.0198,0.0892,0.0312,0.956,2509,10035,80% smallest removed,0.2,"Significant degradation"
PS-006,grok_alpha_trained,925,Lurra Venn,attention,W_Q,16384,16384,0.0,0.0178,0.0134,0.0756,0.0,0.997,16384,0,Unpruned attention query,1.0,"All attention weights active"
PS-007,grok_alpha_trained,925,Lurra Venn,attention,W_Q,16384,13107,0.2,0.0223,0.0145,0.0756,0.0067,0.996,13107,3277,20% pruned,0.8,"Attention robust to pruning"
PS-008,grok_alpha_trained,925,Lurra Venn,attention,W_Q,16384,9830,0.4,0.0289,0.0156,0.0756,0.0112,0.993,9830,6554,40% pruned,0.6,"Moderate sparsity"
PS-009,grok_alpha_trained,925,Lurra Venn,attention,W_K,16384,16384,0.0,0.0189,0.0145,0.0823,0.0,0.997,16384,0,Unpruned attention key,1.0,"All key weights active"
PS-010,grok_alpha_trained,925,Lurra Venn,attention,W_V,16384,16384,0.0,0.0198,0.0156,0.0789,0.0,0.997,16384,0,Unpruned attention value,1.0,"All value weights active"
PS-011,grok_alpha_trained,925,Lurra Venn,mlp,W_in,16384,16384,0.0,0.0212,0.0167,0.0934,0.0,0.997,16384,0,Unpruned MLP input,1.0,"MLP fully active"
PS-012,grok_alpha_trained,925,Lurra Venn,mlp,W_in,16384,8192,0.5,0.0398,0.0189,0.0934,0.0189,0.991,8192,8192,50% pruned - interesting pattern,0.5,"Half weights carry most information"
PS-013,grok_alpha_trained,925,Lurra Venn,mlp,W_out,16384,16384,0.0,0.0189,0.0145,0.0867,0.0,0.997,16384,0,Unpruned MLP output,1.0,"MLP output fully active"
PS-014,grok_alpha_trained,925,Lurra Venn,unembedding,W_U,12544,12544,0.0,0.0256,0.0178,0.0945,0.0,0.997,12544,0,Unpruned unembedding,1.0,"All unembed weights active"
PS-015,grok_alpha_trained,925,Lurra Venn,unembedding,W_U,12544,6272,0.5,0.0467,0.0198,0.0945,0.0234,0.982,6272,6272,50% pruned - critical layer,0.5,"Unembedding sensitive to pruning"
PS-016,grok_beta_pretrain,925,Lurra Venn,embedding,W_E,12544,12544,0.0,0.0156,0.0312,0.1234,0.0,0.012,12544,0,Pre-grokking embedding,1.0,"Before grokking - random structure"
PS-017,grok_beta_pretrain,925,Lurra Venn,mlp,W_in,16384,16384,0.0,0.0145,0.0289,0.1456,0.0,0.012,16384,0,Pre-grokking MLP,1.0,"Random weights before training"
PS-018,grok_gamma_early,925,Lurra Venn,embedding,W_E,12544,12544,0.0,0.0178,0.0256,0.1012,0.0,0.956,12544,0,Memorization phase,1.0,"Memorized but not generalized"
PS-019,grok_gamma_early,925,Lurra Venn,embedding,W_E,12544,5018,0.6,0.0389,0.0198,0.1012,0.0178,0.234,5018,7526,60% pruned memorizer,0.4,"Pruning destroys memorization"
PS-020,fourier_analysis,925,Tobben Rast,embedding,fourier_k14,1024,1024,0.0,0.0456,0.0123,0.0789,0.0,0.997,1024,0,Key frequency 14 - unpruned,1.0,"Frequency 14 critical for mod 97"
PS-021,fourier_analysis,925,Tobben Rast,embedding,fourier_k14,1024,512,0.5,0.0678,0.0145,0.0789,0.0234,0.678,512,512,50% of frequency 14 pruned,0.5,"Significant accuracy loss"
PS-022,fourier_analysis,925,Tobben Rast,embedding,fourier_k7,1024,1024,0.0,0.0234,0.0089,0.0456,0.0,0.997,1024,0,Frequency 7 - unpruned,1.0,"Secondary frequency"
PS-023,fourier_analysis,925,Tobben Rast,embedding,fourier_k7,1024,512,0.5,0.0345,0.0098,0.0456,0.0145,0.892,512,512,50% of frequency 7 pruned,0.5,"Moderate accuracy loss"
PS-024,ghost_persistence,925,Lurra Venn,full_network,all_layers,50000,50000,0.0,0.0198,0.0178,0.1234,0.0,0.997,50000,0,Full network,1.0,"Complete ghost persistence"
PS-025,ghost_persistence,925,Lurra Venn,full_network,all_layers,50000,25000,0.5,0.0356,0.0189,0.1234,0.0156,0.989,25000,25000,Half network remains,0.5,"50% ghost remains"
PS-026,ghost_persistence,925,Lurra Venn,full_network,all_layers,50000,10000,0.8,0.0589,0.0198,0.1234,0.0278,0.934,10000,40000,20% network remains,0.2,"20% ghost - barely functional"
PS-027,ghost_persistence,925,Lurra Venn,full_network,all_layers,50000,5000,0.9,0.0789,0.0212,0.1234,0.0356,0.756,5000,45000,10% network remains,0.1,"10% ghost - degraded"
PS-028,cleanup_phase,925,Tobben Rast,embedding,W_E,12544,12544,0.0,0.0567,0.0234,0.1456,0.0,0.456,12544,0,Pre-cleanup (epoch 5000),1.0,"Before cleanup - high variance"
PS-029,cleanup_phase,925,Tobben Rast,embedding,W_E,12544,12544,0.0,0.0345,0.0156,0.0892,0.0,0.923,12544,0,Mid-cleanup (epoch 8000),1.0,"Cleanup reducing magnitude"
PS-030,cleanup_phase,925,Tobben Rast,embedding,W_E,12544,12544,0.0,0.0234,0.0089,0.0567,0.0,0.997,12544,0,Post-cleanup (epoch 12000),1.0,"Clean sparse representation"
