component_id,component_name,categorical_interpretation,prior_scholar,prior_concept,year_synthesized,synthesizer,layer_position,data_flow_description,mathematical_structure,example_operation,connection_to_ml,archive_reference
TC-001,token_embedding,functor_to_vector_space,lorren_dray,document_functor,978,brennet_kell,input,Maps discrete tokens to continuous vector representations,F: Tokens → Vec,word → [0.2 -0.5 0.8 ...],Embedding layer in transformers,BKL-001-1
TC-002,positional_encoding,qk_twist_mechanism,mira_vash,qk_twist,978,brennet_kell,input,Adds position information to token embeddings via rotation,Twist: Vec × Pos → Vec,embed + sin/cos(pos),Sinusoidal or learned position encodings,BKL-001-2
TC-003,query_projection,morphism_source_representation,merrit_vance,query_key_structure,982,brennet_kell,attention_input,Projects embedding to query space for attention computation,W_Q: Vec → Vec_Q,x → W_Q · x,Query projection matrix,BKL-001-3
TC-004,key_projection,morphism_target_representation,merrit_vance,query_key_structure,982,brennet_kell,attention_input,Projects embedding to key space for attention matching,W_K: Vec → Vec_K,x → W_K · x,Key projection matrix,BKL-001-4
TC-005,value_projection,information_carrier,merrit_vance,weighted_passages,982,brennet_kell,attention_input,Projects embedding to value space for information retrieval,W_V: Vec → Vec_V,x → W_V · x,Value projection matrix,BKL-001-5
TC-006,attention_scores,enriched_morphism_weights,merrit_vance,attention_as_weight,982,brennet_kell,attention_core,Computes relevance between query and all keys,Score: Q × K^T,dot(q_i k_j) / sqrt(d),Raw attention logits,BKL-001-6
TC-007,softmax_normalization,normalization_functor,merrit_vance,normalization_principle,982,brennet_kell,attention_core,Converts scores to probability distribution summing to one,Softmax: R^n → Simplex,exp(x_i) / sum(exp),Softmax function,BKL-001-7
TC-008,attention_output,weighted_value_composition,merrit_vance,weighted_passage_composition,982,brennet_kell,attention_output,Sums values weighted by attention scores,Sum(a_i · v_i),weighted average of values,Attention head output,BKL-001-8
TC-009,multi_head_attention,parallel_probing,pelleth_strand,probing_lemma,985,brennet_kell,attention_layer,Multiple attention heads probe different aspects in parallel,Head_1 || Head_2 || ... || Head_h,concat(head_1 ... head_h),Multi-head attention,BKL-001-9
TC-010,head_concatenation,probe_aggregation,pelleth_strand,yoneda_embedding,985,brennet_kell,attention_output,Concatenates outputs from all heads,Concat: Vec^h → Vec,stack all head outputs,Concatenation before output projection,BKL-001-10
TC-011,output_projection,representation_compression,gellen_tross,natural_transformation,985,brennet_kell,attention_output,Projects concatenated heads back to model dimension,W_O: Vec_concat → Vec,concat → W_O · concat,Output projection matrix,BKL-001-11
TC-012,residual_connection,information_stream_preservation,dellan_korr,residual_stream,978,brennet_kell,layer_structure,Adds input directly to sublayer output for gradient flow,x + Sublayer(x),skip connection,Residual connections,BKL-001-12
TC-013,layer_normalization,distribution_stabilization,merrit_vance,normalization_principle,988,brennet_kell,layer_structure,Normalizes activations to zero mean unit variance,LayerNorm: Vec → Vec_normalized,normalize each position independently,LayerNorm,BKL-001-13
TC-014,feed_forward_layer,pointwise_transformation,tessery_vold,morphism_composition,988,brennet_kell,ffn_layer,Applies two linear transformations with nonlinearity,FFN: Vec → Vec,max(0 W_1·x + b_1)·W_2 + b_2,Position-wise FFN,BKL-001-14
TC-015,transformer_block,enriched_category_layer,brennet_kell,transformer_as_enriched_category,992,brennet_kell,full_layer,Complete transformer layer: attention + residual + FFN + residual,Block: Vec^n → Vec^n,one full transformer layer,Transformer block,BKL-001-15
TC-016,layer_stack,iterated_enriched_composition,brennet_kell,layer_composition,992,brennet_kell,architecture,Stacks L transformer blocks for deep processing,Block_1 ∘ Block_2 ∘ ... ∘ Block_L,composition of layers,Stacked transformer layers,BKL-001-16
TC-017,context_window,morphism_domain_bound,corren_moll,distributional_hypothesis,988,brennet_kell,architecture,Limits attention to fixed context length,Dom: {1..max_len},attention mask for context,Maximum context length,BKL-001-17
TC-018,causal_mask,morphism_direction_constraint,tessery_vold,morphism_antisymmetry,992,brennet_kell,attention_core,Prevents attention to future positions in autoregressive models,Mask: triangular matrix,zero future attention,Causal/autoregressive masking,BKL-001-18
TC-019,output_head,terminal_functor,lorren_dray,representable_perspective,995,brennet_kell,output,Maps final representations to vocabulary distribution,F: Vec → P(Vocab),final linear + softmax,Language modeling head,BKL-001-19
TC-020,full_transformer,enriched_categorical_architecture,brennet_kell,transformer_discipline,998,brennet_kell,complete,Complete transformer: embed → blocks → output,Transformer: Tokens^n → P(Vocab)^n,full forward pass,Complete transformer model,BKL-002-1
