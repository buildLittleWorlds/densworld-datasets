experiment_id,experiment_name,year,experimenter,model_name,model_type,hidden_dim,num_layers,extraction_method,dictionary_size,sparsity_penalty,reconstruction_loss,sparsity_achieved,num_features_found,num_interpretable,interpretability_rate,feature_categories,monosemantic_count,polysemantic_count,dead_features,mean_activation_frequency,max_activation_frequency,min_activation_frequency,training_epochs,learning_rate,batch_size,convergence_status,densworld_analog,notes
FE-001,Baseline Extraction,921,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.023,0.85,487,312,0.641,arithmetic|position|frequency,298,189,25,0.034,0.892,0.001,5000,0.0003,64,converged,Form Library initial catalog,First systematic extraction attempt
FE-002,High Sparsity Trial,921,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.05,0.067,0.92,445,289,0.649,arithmetic|frequency,267,178,67,0.021,0.734,0.0001,5000,0.0003,64,converged,Pure form isolation,Higher sparsity increases interpretability slightly
FE-003,Low Sparsity Trial,921,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.001,0.008,0.45,498,198,0.398,mixed|entangled,145,353,14,0.089,0.967,0.012,5000,0.0003,64,converged,Tangled archive state,Low sparsity leads to polysemantic features
FE-004,Large Dictionary,922,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,1024,0.01,0.019,0.88,978,687,0.702,arithmetic|position|frequency|residual,623,355,46,0.028,0.845,0.0005,8000,0.0003,64,converged,Expanded catalog,Larger dictionary improves coverage
FE-005,Small Dictionary,922,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,256,0.01,0.041,0.78,234,156,0.667,arithmetic|position,142,92,22,0.052,0.923,0.003,5000,0.0003,64,converged,Compressed catalog,Smaller dictionary forces feature sharing
FE-006,Pre-Grokking Model,922,Prenn Vash,grok_beta_pretrain,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.089,0.67,423,89,0.210,memorization|position,45,378,89,0.067,0.945,0.002,5000,0.0003,64,converged,Pre-clarity archive,Features before grokking are mostly memorization
FE-007,Post-Grokking Model,922,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.023,0.85,487,312,0.641,arithmetic|frequency,298,189,25,0.034,0.892,0.001,5000,0.0003,64,converged,Post-clarity archive,Features after grokking are algorithmic
FE-008,Fourier Feature Search,922,Prenn Vash,grok_alpha,transformer_1layer,128,1,targeted_probe,na,na,na,na,97,97,1.000,frequency,97,0,0,0.156,0.934,0.023,na,na,na,complete,Resonance patterns,Targeted search for known Fourier components
FE-009,Residual Stream Analysis,923,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.031,0.82,467,234,0.501,position|arithmetic|attention,198,269,45,0.041,0.867,0.002,5000,0.0003,64,converged,Flow analysis,Residual stream contains mixed information
FE-010,MLP Analysis,923,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.027,0.84,478,298,0.623,computation|frequency,278,200,34,0.038,0.889,0.001,5000,0.0003,64,converged,Computation center,MLP implements the core algorithm
FE-011,Attention Pattern Analysis,923,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.034,0.79,456,267,0.585,position|routing,234,222,56,0.045,0.912,0.003,5000,0.0003,64,converged,Routing patterns,Attention moves information between positions
FE-012,Cross-Task Transfer,923,Prenn Vash,grok_gamma_mult,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.028,0.83,489,287,0.587,multiplication|frequency,265,224,23,0.036,0.878,0.001,5000,0.0003,64,converged,Cross-domain mapping,Features partially transfer between tasks
FE-013,Feature Stability Test,923,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.024,0.84,491,309,0.629,arithmetic|frequency,294,197,21,0.035,0.891,0.001,5000,0.0003,64,converged,Temporal stability,Features stable across random seeds
FE-014,TopK Activation,924,Prenn Vash,grok_alpha,transformer_1layer,128,1,topk_autoencoder,512,na,0.021,0.91,498,334,0.671,arithmetic|position|frequency,312,186,14,0.029,0.856,0.002,5000,0.0003,64,converged,Selective activation,TopK constraint improves feature quality
FE-015,Gated SAE,924,Prenn Vash,grok_alpha,transformer_1layer,128,1,gated_sae,512,0.01,0.018,0.87,502,356,0.709,arithmetic|frequency|position,334,168,10,0.031,0.867,0.001,6000,0.0003,64,converged,Gated extraction,Gated architecture reduces dead features
FE-016,Layer 0 Embedding,924,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.015,0.89,487,378,0.776,token|position,356,131,24,0.033,0.945,0.002,5000,0.0003,64,converged,Input encoding,Embedding layer features are highly interpretable
FE-017,Attention Output,924,Prenn Vash,grok_alpha,transformer_1layer,128,1,sparse_autoencoder,512,0.01,0.029,0.81,467,256,0.548,routing|composition,234,233,46,0.042,0.889,0.002,5000,0.0003,64,converged,Attention output,Mixed information after attention
FE-018,Progressive Extraction,924,Prenn Vash,grok_alpha,transformer_1layer,128,1,progressive_sae,512,0.01,0.020,0.86,495,345,0.697,arithmetic|frequency,323,172,17,0.032,0.878,0.001,7000,0.0003,64,converged,Layered extraction,Progressive training improves results
FE-019,Multi-Scale Analysis,924,Prenn Vash,grok_alpha,transformer_1layer,128,1,multiscale_sae,1024,0.01,0.017,0.88,987,712,0.721,fine|coarse|arithmetic,678,309,31,0.027,0.834,0.0003,8000,0.0003,64,converged,Multi-resolution,Different scales capture different features
FE-020,Anthropic Replication,925,Prenn Vash,grok_alpha,transformer_1layer,128,1,anthropic_sae,4096,0.01,0.012,0.91,3912,2845,0.727,arithmetic|position|frequency|misc,2567,1345,184,0.019,0.812,0.00001,10000,0.0001,128,converged,Full catalog,Large-scale extraction following Anthropic method
FE-021,Feature Geometry Study,925,Prenn Vash,grok_alpha,transformer_1layer,128,1,geometric_analysis,512,0.01,0.023,0.85,487,312,0.641,geometric|angular,na,na,na,na,na,na,na,na,na,complete,Geometric mapping,Studying feature vector relationships
FE-022,Activation Patching,925,Prenn Vash,grok_alpha,transformer_1layer,128,1,activation_patching,na,na,na,na,156,134,0.859,causal,na,na,na,na,na,na,na,na,na,complete,Causal tracing,Identifying causally important features
FE-023,Feature Attribution,925,Prenn Vash,grok_alpha,transformer_1layer,128,1,integrated_gradients,na,na,na,na,234,189,0.808,attribution,na,na,na,na,na,na,na,na,na,complete,Attribution mapping,Feature importance via gradients
FE-024,Concept Erasure,925,Prenn Vash,grok_alpha,transformer_1layer,128,1,concept_erasure,na,na,na,na,45,45,1.000,erasure,na,na,na,na,na,na,na,na,na,complete,Concept removal,Testing feature necessity
FE-025,Comparison Study,925,Prenn Vash,multiple,various,various,various,comparison,512,0.01,varies,varies,varies,varies,varies,comparison,na,na,na,na,na,na,5000,0.0003,64,complete,Method comparison,Comparing extraction methods
